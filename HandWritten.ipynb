{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train = True,\n",
    "    transform=ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train = False,\n",
    "    transform=ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    'test': DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x1a11d5e6310>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x1a11dd67550>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_21064\\2530090726.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303684\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.203210\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 1.970510\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 1.983889\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.839723\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.842676\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.848570\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 1.790186\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.694074\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.719970\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.717210\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 1.749731\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.705507\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 1.731075\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.828532\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.623957\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.791471\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 1.672251\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.716316\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 1.715212\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.694636\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 1.704772\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 1.759585\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 1.688595\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.621907\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.722623\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 1.776119\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 1.640778\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.675534\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 1.686076\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9301/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.678499\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 1.751184\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 1.616517\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 1.704719\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1.639505\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.743858\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 1.695970\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 1.638314\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.664764\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 1.670803\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.630803\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 1.648906\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 1.674271\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 1.660364\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 1.687190\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.694816\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.641026\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 1.692078\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 1.801362\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 1.722272\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.711571\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 1.745813\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 1.670034\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 1.654473\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.751142\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.679287\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 1.780374\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 1.790837\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 1.671162\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 1.731563\n",
      "\n",
      "Test set: Average loss: 0.0156, Accuracy: 8991/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.695303\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 1.699122\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 1.660396\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 1.691148\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 1.741228\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.669692\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 1.721120\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 1.644762\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.720937\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 1.851363\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.741167\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 1.701340\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 1.850983\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 1.771148\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 1.681127\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.791108\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.736201\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 1.711148\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 1.871242\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 1.634985\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.719997\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 1.695008\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 1.821147\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 1.791147\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.771082\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.807445\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 1.730652\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 1.663646\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 1.671150\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 1.707046\n",
      "\n",
      "Test set: Average loss: 0.0161, Accuracy: 8476/10000 (85%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.782779\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 1.741484\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 1.720870\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 1.711150\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 1.631168\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.739500\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 1.691149\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 1.671342\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 1.711150\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 1.731150\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.794730\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 1.781150\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 1.791152\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 1.761151\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 1.770807\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.721750\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.641148\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 1.754638\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 1.731148\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 1.631150\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.701150\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 1.661512\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 1.741169\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 1.661148\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.691150\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.730894\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 1.731146\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 1.761157\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 1.719726\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 1.771987\n",
      "\n",
      "Test set: Average loss: 0.0158, Accuracy: 8836/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.751142\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 1.780920\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 1.701151\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 1.741147\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 1.730991\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.731136\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 1.701139\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 1.821265\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 1.731149\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 1.691233\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.711182\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 1.681035\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 1.613620\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 1.701150\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 1.740231\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.691161\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.661139\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 1.651152\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 1.661150\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 1.651198\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.731591\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 1.661150\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 1.731005\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 1.761150\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 1.680178\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.713519\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 1.741131\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 1.741148\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 1.771149\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 1.684976\n",
      "\n",
      "Test set: Average loss: 0.0160, Accuracy: 8616/10000 (86%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.741150\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 1.751938\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 1.730251\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 1.711150\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 1.671149\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.741150\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 1.781165\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 1.711150\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 1.720535\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 1.781150\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.701150\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 1.701136\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 1.821146\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 1.800826\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 1.771473\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.721150\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.751150\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 1.771148\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 1.751150\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 1.671151\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.631150\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 1.744815\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 1.711272\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 1.771156\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 1.771150\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.789778\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 1.830979\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 1.771150\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 1.721150\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 1.730753\n",
      "\n",
      "Test set: Average loss: 0.0156, Accuracy: 8995/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.876718\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 1.721150\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 1.715725\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 1.891150\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 1.801151\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.891150\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 1.841150\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 1.801133\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 1.761149\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 1.701150\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.721150\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 1.811150\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 1.751150\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 1.751150\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 1.781150\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.750987\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.731150\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 1.871150\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 1.781150\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 1.800966\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.791150\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 1.821150\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 1.781103\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 1.821150\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 1.819885\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.841151\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 1.771150\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 1.821150\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 1.801150\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 1.741150\n",
      "\n",
      "Test set: Average loss: 0.0164, Accuracy: 8193/10000 (82%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.751150\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 1.891150\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 1.814244\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 1.761147\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 1.632384\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.791145\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 1.800692\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 1.821027\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 1.821142\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 1.791150\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.850959\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 1.751150\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 1.801150\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 1.821150\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 1.751150\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.861150\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.641150\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 1.871150\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 1.691150\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 1.661150\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.801150\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 1.781150\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 1.711150\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 1.771150\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 1.761150\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.791150\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 1.891948\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 1.741150\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 1.731150\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 1.781150\n",
      "\n",
      "Test set: Average loss: 0.0157, Accuracy: 8878/10000 (89%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.751150\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 1.680913\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 1.728030\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 1.771150\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 1.881150\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.721150\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 1.901150\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 1.831085\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 1.791150\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 1.711150\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.771144\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 1.871150\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 1.761150\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 1.771150\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 1.891942\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.941153\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.821150\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 1.791150\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 1.881150\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 1.821150\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.781150\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 1.771150\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 1.751150\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 1.771150\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 1.721111\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.761150\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 1.811150\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 1.830376\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 1.861150\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 1.841150\n",
      "\n",
      "Test set: Average loss: 0.0160, Accuracy: 8562/10000 (86%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.801150\n",
      "Train Epoch: 10 [2000/60000 (3%)]\tLoss: 1.730943\n",
      "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 1.831150\n",
      "Train Epoch: 10 [6000/60000 (10%)]\tLoss: 1.691150\n",
      "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 1.781150\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 1.691150\n",
      "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 1.726178\n",
      "Train Epoch: 10 [14000/60000 (23%)]\tLoss: 1.721150\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 1.711997\n",
      "Train Epoch: 10 [18000/60000 (30%)]\tLoss: 1.811150\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.961150\n",
      "Train Epoch: 10 [22000/60000 (37%)]\tLoss: 1.873999\n",
      "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 1.831150\n",
      "Train Epoch: 10 [26000/60000 (43%)]\tLoss: 1.751150\n",
      "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 1.801233\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.861465\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.791150\n",
      "Train Epoch: 10 [34000/60000 (57%)]\tLoss: 1.821151\n",
      "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 1.831150\n",
      "Train Epoch: 10 [38000/60000 (63%)]\tLoss: 1.771150\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.700158\n",
      "Train Epoch: 10 [42000/60000 (70%)]\tLoss: 1.711150\n",
      "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 1.761156\n",
      "Train Epoch: 10 [46000/60000 (77%)]\tLoss: 1.741150\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 1.810027\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.811150\n",
      "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 1.771150\n",
      "Train Epoch: 10 [54000/60000 (90%)]\tLoss: 1.781150\n",
      "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 1.801150\n",
      "Train Epoch: 10 [58000/60000 (97%)]\tLoss: 1.771150\n",
      "\n",
      "Test set: Average loss: 0.0159, Accuracy: 8757/10000 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_21064\\2530090726.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "data, target = test_data[1]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "print(f'Prediction: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4549, 0.4902, 0.6706, 1.0000, 1.0000, 0.5882,\n",
       "           0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.6627, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.8549, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6627, 0.9922, 0.9922, 0.9922, 0.8353, 0.5569, 0.6902, 0.9922,\n",
       "           0.9922, 0.4784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2039,\n",
       "           0.9804, 0.9922, 0.8235, 0.1255, 0.0471, 0.0000, 0.0235, 0.8078,\n",
       "           0.9922, 0.5490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3020,\n",
       "           0.9843, 0.8235, 0.0980, 0.0000, 0.0000, 0.0000, 0.4784, 0.9725,\n",
       "           0.9922, 0.2549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1216, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.8196, 0.9922,\n",
       "           0.9922, 0.2549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4588, 0.9686, 0.9922,\n",
       "           0.7765, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.2980, 0.9686, 0.9922, 0.9059,\n",
       "           0.2471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.5020, 0.9922, 0.9922, 0.5647,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.6902, 0.9647, 0.9922, 0.6235, 0.0471,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0980, 0.9176, 0.9922, 0.9137, 0.1373, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7765, 0.9922, 0.9922, 0.5529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3059, 0.9725, 0.9922, 0.7412, 0.0471, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0745, 0.7843, 0.9922, 0.9922, 0.5529, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.5255, 0.9922, 0.9922, 0.6784, 0.0471, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.9725, 0.9922, 0.9922, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.9725, 0.9922, 0.9922, 0.1686, 0.0784, 0.0784, 0.0784, 0.0784,\n",
       "           0.0196, 0.0000, 0.0196, 0.0784, 0.0784, 0.1451, 0.5882, 0.5882,\n",
       "           0.5882, 0.5765, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.9725, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.6588, 0.5608, 0.6510, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.4824, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6824, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9922, 0.9765, 0.9686, 0.9686, 0.6627,\n",
       "           0.4588, 0.4588, 0.2235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.4627, 0.4824, 0.4824, 0.4824, 0.6510, 0.9922, 0.9922,\n",
       "           0.9922, 0.6078, 0.4824, 0.4824, 0.1608, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
